{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Example for converting patient data to instruction data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook demonstrates the core workflow for converting raw clinical data into **Instruction Tuning** examples for the TwinWeaver model.\n",
    "\n",
    "We will walk through the process of:\n",
    "1.  **Loading Data**: Importing raw tabular data (longitudinal events and static demographics).\n",
    "2.  **Configuration**: Setting up the pipeline to match your data schema.\n",
    "3.  **Splitting**: Generating \"Splits\" (input/output samples) from a patient's timeline.\n",
    "4.  **Conversion**: Transforming these splits into text-based **Instruction** (Input) and **Answer** (Target) pairs suitable for fine-tuning an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from twinweaver import (\n",
    "    DataManager,\n",
    "    Config,\n",
    "    DataSplitterForecasting,\n",
    "    DataSplitterEvents,\n",
    "    ConverterInstruction,\n",
    "    DataSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Basic Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We require three standardized dataframes to construct the patient digital twin:\n",
    "\n",
    "* **Events (`df_events`)**: The longitudinal history in 'long' format (one row per event). Required columns:\n",
    "    * `patientid`: Unique patient identifier.\n",
    "    * `date`: Date of the event.\n",
    "    * `event_category`: High-level grouping (e.g., 'lab', 'drug', 'condition', 'lot').\n",
    "    * `event_name`: Specific variable name (e.g., 'Hemoglobin', 'Metformin').\n",
    "    * `event_value`: The result/value (e.g., '12.5', 'Start').\n",
    "    * `event_descriptive_name`: Natural language description used in the text prompt.\n",
    "* **Constant (`df_constant`)**: Static patient information (one row per patient). Contains demographics like birth year, gender, and histology.\n",
    "* **Constant Description (`df_constant_description`)**: Metadata mapping constant columns to natural language descriptions. Columns: `variable`, `comment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - generated example data\n",
    "df_events = pd.read_csv(\"./example_data/events.csv\")\n",
    "df_constant = pd.read_csv(\"./example_data/constant.csv\")\n",
    "df_constant_description = pd.read_csv(\"./example_data/constant_description.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Configuration and Data Manager\n",
    "\n",
    "We initialize the `Config` object, which serves as the central control for column mapping, token limits, and prompt templates. You can override defaults here to match your specific dataset schema (e.g., specifying which columns in `df_constant` to include).\n",
    "\n",
    "The `DataManager` then ingests the raw dataframes, handling preprocessing steps like date parsing, unique event mapping, and train/test splitting at the patient level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()  # Override values here to customize pipeline\n",
    "config.constant_columns_to_use = [\n",
    "    \"birthyear\",\n",
    "    \"gender\",\n",
    "    \"histology\",\n",
    "    \"smoking_history\",\n",
    "]  # Manually set from constant DF\n",
    "config.constant_birthdate_column = \"birthyear\"\n",
    "\n",
    "\n",
    "dm = DataManager(config=config)\n",
    "dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\n",
    "dm.process_indication_data()\n",
    "dm.setup_unique_mapping_of_events()\n",
    "dm.setup_dataset_splits()\n",
    "dm.infer_var_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Initialize Splitters and Converter\n",
    "\n",
    "To generate diverse training examples, we use specialized **Data Splitters**:\n",
    "* `DataSplitterEvents`: Identifies time points for predicting discrete outcomes (e.g., progression, death).\n",
    "* `DataSplitterForecasting`: Identifies time points for forecasting continuous variables (e.g., future lab values).\n",
    "\n",
    "The `ConverterInstruction` is the core engine that transforms these data points into tokenized text. It respects a token budget (e.g., 8192 tokens) to ensure the generated prompts fit within the model's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data splitter handles event prediction tasks\n",
    "data_splitter_events = DataSplitterEvents(dm, config=config)\n",
    "data_splitter_events.setup_variables()\n",
    "\n",
    "# This data splitter handles forecasting tasks\n",
    "data_splitter_forecasting = DataSplitterForecasting(\n",
    "    data_manager=dm,\n",
    "    config=config,\n",
    ")\n",
    "# If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step\n",
    "data_splitter_forecasting.setup_statistics()\n",
    "\n",
    "# We will also use the easier interface that combines both data splitters\n",
    "data_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n",
    "\n",
    "# Set up the converter instruction\n",
    "converter = ConverterInstruction(\n",
    "    nr_tokens_budget_total=8192,\n",
    "    config=config,\n",
    "    dm=dm,\n",
    "    variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Examine patient data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "From the data manager we can get the patient, for example this patientid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "patientid = dm.all_patientids[4]\n",
    "patientid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Let's checkout the data of the patient. `patient_data` is a dictionary containing the patient's data, with two keys: \n",
    "- `\"events\"`: A pandas DataFrame containing all time-series events\n",
    "                        (original events and molecular data combined and sorted\n",
    "                        by date).\n",
    "- `\"constant\"`: A pandas DataFrame containing the static (constant)\n",
    "                data for the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data = dm.get_patient_data(patientid)\n",
    "patient_data[\"events\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data[\"constant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Convert patient data to string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Generate Training Splits\n",
    "\n",
    "A single patient's timeline can yield multiple training examples. A **Split** represents a specific point in time (the \"split date\") where we divide the data:\n",
    "* **Input**: History *before* the split date.\n",
    "* **Target**: Future events or values *after* the split date.\n",
    "\n",
    "The `get_splits_from_patient_with_target` method samples valid split points (anchored to sampling random times around the lines of therapy) and determines appropriate targets (forecasting vs. event prediction). This allows the model to learn from various stages of a patient's journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(\n",
    "    patient_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Now for each split, we can generate these strings. We just pick the first one as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = 0\n",
    "p_converted = converter.forward_conversion(\n",
    "    forecasting_splits=forecasting_splits[split_idx],\n",
    "    event_splits=events_splits[split_idx],\n",
    "    override_mode_to_select_forecasting=\"both\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Inspect the Output\n",
    "\n",
    "The `forward_conversion` method returns `p_converted`, a dictionary containing the final LLM training example:\n",
    "\n",
    "* **`instruction`**: The full text prompt. It includes the patient's history (demographics + events) followed by the specific task questions.\n",
    "* **`answer`**: The target completion string containing the correct answers for the tasks.\n",
    "* **`meta`**: Structured metadata used to generate the text, useful for debugging or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_converted[\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_converted[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_converted[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Reverse Conversion: Text to Structured Data\n",
    "\n",
    "Finally, we demonstrate the **Reverse Conversion** process. This is the inverse of the instruction generation step. It takes the text string (which would be generated by the model during inference) and parses it back into structured Pandas DataFrames.\n",
    "\n",
    "This capability is crucial for:\n",
    "* **Evaluation**: Comparing the model's text predictions against ground truth data programmatically.\n",
    "* **Integration**: Converting the model's narrative outputs back into downstream clinical systems or dashboards.\n",
    "\n",
    "In this example, we take the `answer` string we just generated and confirm it can be reconstructed into a structured dataframe using the `reverse_conversion` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = reference_dates[\"date\"][0]\n",
    "return_list = converter.reverse_conversion(p_converted[\"answer\"], dm, date)\n",
    "return_list[0][\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
