{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# End to end instruction example using LLMs with fine-tuning\n",
    "\n",
    "This notebook provides a comprehensive, end-to-end demonstration of fine-tuning a Large Language Model (LLM) for medical forecasting tasks using the twinweaver library. The workflow begins by processing raw medical data (events, constants, and lab values) into structured instruction-tuning datasets using DataManager and ConverterInstruction, effectively translating patient histories into prompt-completion pairs. We then implement Parameter-Efficient Fine-Tuning (PEFT) using QLoRA (4-bit quantization) and the SFTTrainer to adapt a microsoft/Phi-4-mini-instruct model, optimizing it for clinical predictions while managing memory constraints. Finally, the example concludes with an inference pipeline that loads the trained adapter to predict future clinical outcomes—such as hemoglobin levels and mortality risks—and reverse-converts the LLM's text output back into structured data.\n",
    "\n",
    "> **Note:** You need a GPU with at least 30GB of memory for this example to work.\n",
    "We also have not tested the performance of PEFT models - only as examples.\n",
    "\n",
    "> **Important:** Please install first the fine-tuning packages with `pip install twinweaver[fine-tuning-example]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "\n",
    "from twinweaver import (\n",
    "    DataManager,\n",
    "    Config,\n",
    "    DataSplitterForecasting,\n",
    "    DataSplitterEvents,\n",
    "    ConverterInstruction,\n",
    "    DataSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some key settings\n",
    "BASE_MODEL = \"microsoft/Phi-4-mini-instruct\"  # NOTE: we haven't tested the performance of this model beyond examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "First, we need to set up the configuration. This includes specifying which constant variables to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_events = pd.read_csv(\"./example_data/events.csv\")\n",
    "df_constant = pd.read_csv(\"./example_data/constant.csv\")\n",
    "df_constant_description = pd.read_csv(\"./example_data/constant_description.csv\")\n",
    "\n",
    "# Manually set up which constant columns we want to use\n",
    "config = Config()  # Override values here to customize pipeline\n",
    "config.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]\n",
    "config.constant_birthdate_column = \"birthyear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Here we initialize the `DataManager` to handle data loading and processing. \n",
    "\n",
    "We also set up the Data Splitters:\n",
    "*   `DataSplitterEvents`: Handles splitting of event data (diagnoses, treatments).\n",
    "*   `DataSplitterForecasting`: Handles splitting of time-series data (lab values) and statistics generation.\n",
    "\n",
    "Finally, `ConverterInstruction` is initialized. This component is responsible for translating the structured patient data splits into the textual instruction format (Prompt + Completion) that the LLM understands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager(config=config)\n",
    "dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\n",
    "dm.process_indication_data()\n",
    "dm.setup_unique_mapping_of_events()\n",
    "dm.setup_dataset_splits()\n",
    "dm.setup_dataset_splits()\n",
    "dm.infer_var_types()\n",
    "\n",
    "data_splitter_events = DataSplitterEvents(dm, config=config)\n",
    "data_splitter_events.setup_variables()\n",
    "\n",
    "data_splitter_forecasting = DataSplitterForecasting(\n",
    "    data_manager=dm,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step\n",
    "data_splitter_forecasting.setup_statistics()\n",
    "\n",
    "# We will also use the easier interface that combines both data splitters\n",
    "data_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n",
    "\n",
    "converter = ConverterInstruction(\n",
    "    nr_tokens_budget_total=8192,\n",
    "    config=config,\n",
    "    dm=dm,\n",
    "    variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all training + validation patientids\n",
    "training_patientids = dm.get_all_patientids_in_split(config.train_split_name)\n",
    "validation_patientids = dm.get_all_patientids_in_split(config.validation_split_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The `generate_transformers_df` function iterates through each patient and generates input/output pairs. \n",
    "For each patient, it may generate multiple \"splits\" (different reference dates in their history). Each split is converted into a text prompt (history) and a text completion (future events/values). \n",
    "The result is a DataFrame with \"prompt\" and \"completion\" columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transformers_df(patientids_list):\n",
    "    df = []\n",
    "\n",
    "    for patientid in patientids_list:\n",
    "        patient_data = dm.get_patient_data(patientid)\n",
    "\n",
    "        forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(\n",
    "            patient_data,\n",
    "            forecasting_filter_outliers=False,\n",
    "        )\n",
    "\n",
    "        for split_idx in range(len(forecasting_splits)):\n",
    "            p_converted = converter.forward_conversion(\n",
    "                forecasting_splits=forecasting_splits[split_idx],\n",
    "                event_splits=events_splits[split_idx],\n",
    "                override_mode_to_select_forecasting=\"both\",\n",
    "            )\n",
    "            new_data = {\n",
    "                \"prompt\": p_converted[\"instruction\"],\n",
    "                \"completion\": p_converted[\"answer\"],\n",
    "                \"patientid\": f\"{patientid}_split{split_idx}\",  # Just for ease of finding later\n",
    "            }\n",
    "            df.append(new_data)\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation dfs\n",
    "df_train = generate_transformers_df(training_patientids)\n",
    "df_validation = generate_transformers_df(validation_patientids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Fine-tune LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We start by setting up the tokenizer. We set the padding token to be the same as the EOS (End of Sequence) token, which is a common practice for causal language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tokenizer and datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Set padding token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "validation_dataset = Dataset.from_pandas(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Instruction-tuned models expect data in a specific conversational format (e.g., User: ... Assistant: ...). \n",
    "We use `format_chat_template` to structure our raw prompt/completion strings into this list-of-messages format using the `user` and `assistant` roles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for chat template\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Convert prompt/completion pairs to proper prompt/completion format\"\"\"\n",
    "    return {\n",
    "        \"prompt\": [{\"role\": \"user\", \"content\": example[\"prompt\"]}],\n",
    "        \"completion\": [{\"role\": \"assistant\", \"content\": example[\"completion\"]}],\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply formatting to datasets\n",
    "train_dataset = train_dataset.map(format_chat_template)\n",
    "validation_dataset = validation_dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We configure 4-bit quantization using `BitsAndBytesConfig` (QLoRA). This significantly lowers memory usage, allowing us to fine-tune the model on consumer GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Quantization Config (4-bit loading)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # This should be set based on your GPU capabilities\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Here we set up Low-Rank Adaptation (LoRA) configuration. `LoraConfig` defines the adapter parameters (rank `r`, `alpha`). we target linear layers (`q_proj`, `k_proj` etc.) which generally yields better results than just attending to query/value projections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,  # Rank (higher = more parameters to train)\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target all linear layers for best performance (specific to Llama architecture)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We define the training arguments in `SFTConfig`. Notice the higher learning rate (`1e-4`) compared to typical full fine-tuning in the GDT paper. We also set `bf16=True` for newer GPUs (Ampere+) to improve training stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=1e-4,  # LR is higher for PEFT, see TwinWeaver paper for full fine-tuning details\n",
    "    fp16=False,  # Use fp16 for T4/V100, bf16 for Ampere and later (A100/3090/4090)\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    save_total_limit=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_length=8192,\n",
    "    packing=False,  # Disable packing for instruction tuning\n",
    "    completion_only_loss=True,  # Only compute loss on assistant responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# Disable cache for training (required for gradient checkpointing)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "    eval_dataset=validation_dataset,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training - takes around 5 mins, depending on hardware\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned adapter\n",
    "adapter_path = \"./results/final_adapter\"\n",
    "trainer.save_model(adapter_path)\n",
    "print(f\"Adapter saved to {adapter_path}\")\n",
    "\n",
    "del trainer\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Inference example\n",
    "\n",
    "Inference example for a test set patient, where we want to make predictions after the first line of therapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first test set patient\n",
    "test_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0]\n",
    "patient_data = dm.get_patient_data(test_patientid)\n",
    "\n",
    "# Lets simulate forecasts for after the first line of therapy\n",
    "df_constant_patient = patient_data[\"constant\"].copy()\n",
    "df_events_patient = patient_data[\"events\"].copy()\n",
    "date_of_first_lot = df_events_patient.loc[\n",
    "    df_events_patient[\"event_category\"] == config.event_category_lot, \"date\"\n",
    "].min()\n",
    "\n",
    "# Only keep data until (and including) first line of therapy\n",
    "df_events_patient = df_events_patient.loc[df_events_patient[\"date\"] <= date_of_first_lot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets forecast hemoglobin at 4, 8, and 12 weeks\n",
    "# and death within 52 weeks\n",
    "forecasting_times_to_predict = {\n",
    "    \"hemoglobin_-_718-7\": [4, 8, 12],\n",
    "}\n",
    "\n",
    "forecast_split, events_split = data_splitter.get_splits_from_patient_inference(\n",
    "    patient_data,\n",
    "    inference_type=\"both\",\n",
    "    forecasting_override_variables_to_predict=[\"hemoglobin_-_718-7\"],\n",
    "    events_override_category=\"death\",\n",
    "    events_override_observation_time_delta=pd.Timedelta(days=52 * 7),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We convert the patient data into an instruction prompt. Unlike training, `forward_conversion_inference` only generates the input prompt (without the target answer), as we want the LLM to generate the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to instruction\n",
    "converted = converter.forward_conversion_inference(\n",
    "    forecasting_split=forecast_split,\n",
    "    forecasting_future_weeks_per_variable=forecasting_times_to_predict,\n",
    "    event_split=events_split,\n",
    "    custom_tasks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "For inference, we load the base model again (clean slate) to avoid any state from training, and then attach the adapter we trained. `PeftModel` handles the integration of the LoRA weights with the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "For inference, we load the base model again (clean slate) and then attach the adapter we trained. `PeftModel` handles the integration of the LoRA weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Base Model again (clean instance)\n",
    "base_model_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,  # Reuse the 4-bit config\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# 2. Load the Saved Adapter\n",
    "# This wraps the base model with the fine-tuned LoRA layers\n",
    "inference_model = PeftModel.from_pretrained(base_model_inference, adapter_path)\n",
    "\n",
    "# 3. Switch to evaluation mode\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text generation pipeline\n",
    "# Re-enable cache for inference\n",
    "inference_model.config.use_cache = True\n",
    "text_gen_pipeline = pipeline(\"text-generation\", model=inference_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with LLM\n",
    "generated_answer = text_gen_pipeline(\n",
    "    [{\"role\": \"user\", \"content\": converted[\"instruction\"]}],\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    "    do_sample=True,  # Using nucleus sampling\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the generated answer\n",
    "generated_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "The raw text output from the model needs to be parsed back into structured data. `reverse_conversion` handles this, returning a list of dictionaries with the predicted results for each task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse convert\n",
    "return_list = converter.reverse_conversion(generated_answer, dm, date_of_first_lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 reverse conversion\n",
    "return_list[0][\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 reverse conversion\n",
    "return_list[1][\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
